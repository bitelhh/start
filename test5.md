 

[https://arxiv.org/abs/1803.08834]: 

------

####  **卷积神经网络介绍**

​          过去几年来，计算机视觉研究主要集中在卷积神经网络（常简称为 ConvNet 或 CNN）上。这些工作已经在广泛的分类和回归任务上实现了新的当前最佳表现。相对而言，尽管这些方法的历史可以追溯到多年前，但对这些系统得到出色结果的方式的理论理解还很滞后。事实上，当前计算机视觉领域的很多成果都是将 CNN 当作黑箱使用，这种做法是有效的，但其有效的原因却非常模糊不清，这严重满足不了科学研究的要求。尤其是这两个可以互补的问题：（1）在被学习的方面（比如卷积核），究竟被学习的是什么？（2）在架构设计方面（比如层的数量、核的数量、池化策略、非线性的选择），为什么某些选择优于另一些选择？这些问题的答案不仅有利于提升我们对 CNN 的科学理解，而且还能提升它们的实用性。

此外，目前实现 CNN 的方法需要大量训练数据，而且设计决策对结果表现有很大的影响。更深度的理论理解应该能减轻对数据驱动的设计的依赖。尽管已有实证研究调查了所实现的网络的运行方式，但到目前为止，这些结果很大程度上还局限在内部处理过程的可视化上，目的是为了理解 CNN 中不同层中发生的情况。

![1](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\1.jpg)

- ​      *总的来说，本章将简要概述计算机视觉领域中所用的最突出的多层架构。需要指出，尽管本章涵盖了文献中最重要的贡献，但却不会对这些架构进行全面概述，因为其它地方已经存在这样的概述了（比如 [17, 56, 90]）。相反，本章的目的是为本报告的剩余部分设定讨论基础，以便我们详细展示和讨论当前对用于视觉信息处理的卷积网络的理解。*

  

- 典型的神经网络由一个输入层、一个输出层和多个隐藏层构成，其中每一层都包含多个单元。

 ![v2-a519b6ca032e1f985e5656b37648651f](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\v2-a519b6ca032e1f985e5656b37648651f.jpeg)

 

1. #### 典型神经网络架构示意图![80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(6)](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(6).jpg)，

自动编码器可以定义为由两个主要部分构成的多层神经网络。第一个部分是编码器，可以将输入数据变换成特征向量；第二个部分是解码器，可将生成的特征向量映射回输入空间。

 

 ![v2-3c54ab0677516d26ab43a125271103e3](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\v2-3c54ab0677516d26ab43a125271103e3.jpeg)

##### 2 典型自动编码器网络的结构

###### 循环神经网络

 ![v2-7e423db7f43d3fa57b7fe18ff280d1dd](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\v2-7e423db7f43d3fa57b7fe18ff280d1dd.jpeg)

于序列输入的任务时，循环神经网络（RNN)是最成功的多层架构之一。RNN 可被视为一种特殊类型的神经网络，其中每个隐藏单元的输入时其当前时间步骤观察到的数据和其前一个时间步骤的状态。

 

 

 ![v2-bb45e00f8874d60b8a22969cb2ea1e3e](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\v2-bb45e00f8874d60b8a22969cb2ea1e3e.jpeg)

 

标准循环神经网络的运算的示意图。每个 RNN 单元的输入都是当前时间步骤的新输入和前一个时间步骤的状态

计算得到新输出，这个输出又可被馈送到多层 RNN 的下一层进行处理,典型 LSTM 单元示意图。该单元的输入是当前时间的输入和前一时间的输入，然后它会返回一个输出并将其馈送给下一时间。LSTM 单元的最终输出由输入门、输出门和记忆单元状态控制。

##### 3 卷积网络

卷积网络（CNN）是一类尤其适合计算机视觉应用的神经网络，因为它们能使用局部操作对表征进行分层抽象。有两大关键的设计思想推动了卷积架构在计算机视觉领域的成功。第一，CNN 利用了图像的 2D 结构，并且相邻区域内的像素通常是高度相关的。因此，CNN 就无需使用所有像素单元之间的一对一连接（大多数神经网络都会这么做），而可以使用分组的局部连接。第二，CNN 架构依赖于特征共享，因此每个通道（即输出特征图）是在所有位置使用同一个过滤器进行卷积而生成的。



![v2-f1d58307e2f32769d57015ed857dca06](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\v2-f1d58307e2f32769d57015ed857dca06.jpeg)

图 2.5：标准卷积网络的结构的示意图，图来自 [93]

 ![v2-6a818c6e1c17dbb9b24d8e257fd685dd](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\v2-6a818c6e1c17dbb9b24d8e257fd685dd.jpeg)

![1](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\1.jpg)

##### 4 生成对抗网络

 

典型的生成对抗网络（GAN）由两个互相竞争的模块或子网络构成，即：生成器网络和鉴别器网络

##### 5 多层网络的训练

如前面讨论的一样，多种多层架构的成功都很大程度上取决于它们的学习过程的成功。其训练过程通常都基于使用梯度下降的误差的反向传播。由于使用简单，梯度下降在训练多层架构上有广泛的应用。

##### 6 简单说说迁移学习 

使用多层架构提取的特征在多种不同数据集和任务上的适用性可以归功于它们的分层性质，表征会在这样的结构中从简单和局部向抽象和全局发展。因此，在其层次结构中的低层级提取的特征往往是多种不同任务共有的特征，因此使得多层结构更容易实现迁移学习。

#####  7 深度神经网络学习

![80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(12)](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(12).jpg)

![80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(10)](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(10).jpg)

![80886v00_CN_Deep_Learning_ebook_pdf_1671345105202](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\80886v00_CN_Deep_Learning_ebook_pdf_1671345105202.jpg)

![80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(3)](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(3).jpg)

![80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(1)](C:\Users\86150\Documents\Tencent Files\3329149485\FileRecv\MobileFile\80886v00_CN_Deep_Learning_ebook_pdf_1671345105202(1).jpg)

##### 8 仍待解决的问题

基于上述讨论，基于可视化的方法存在以下关键研究方向：

 

- 首要的一点：开发使可视化评估更为客观的方法是非常重要的，可以通过引入评估所生成的可视化图像的质量和/或含义的指标来实现。
- 另外，尽管看起来以网络为中心的可视化方法更有前景（因为它们在生成可视化结果上不依赖网络自身），但似乎也有必要标准化它们的评估流程。一种可能的解决方案是使用一个基准来为同样条件下训练的网络生成可视化结果。这样的标准化方法反过来也能实现基于指标的评估，而不是当前的解释性的分析。
- 另一个发展方向是同时可视化多个单元以更好地理解处于研究中的表征的分布式方面，甚至同时还能遵循一种受控式方法。

**以下是基于 ablation study 的方法的潜在研究方向：**

###### 

1. 使用共同的系统性组织的数据集，其中带有计算机视觉领域常见的不同难题（比如视角和光照变化），并且还必需有复杂度更大的类别（比如纹理、部件和目标上的复杂度）。事实上，近期已经出现了这样的数据集 [6]。在这样的数据集上使用 ablation study，加上对所得到的混淆矩阵的分析，可以确定 CNN 架构出错的模式，进而实现更好的理解。

2. 此外，对多个协同的 ablation 对模型表现的影响方式的系统性研究是很受关注的。这样的研究应该能延伸我们对独立单元的工作方式的理解。

3. 最后，这些受控方法是很有前景的未来研究方向；因为相比于完全基于学习的方法，这些方法能让我们对这些系统的运算和表征有更深入的理解。这些有趣的研究方向包括：

   

4. 逐步固定网络参数和分析对网络行为的影响。比如，一次固定一层的卷积核参数（基于当前已有的对该任务的先验知识），以分析所采用的核在每一层的适用性。这个渐进式的方法有望揭示学习的作用，而且也可用作最小化训练时间的初始化方法。

5. 类似地，可以通过分析输入信号的性质（比如信号中的常见内容）来研究网络架构本身的设计（比如层的数量或每层中过滤器的数量）。这种方法有助于让架构达到适宜应用的复杂度。

6. 最后，将受控方法用在网络实现上的同时可以对 CNN 的其它方面的作用进行系统性的研究，由于人们重点关注的所学习的参数，所以这方面得到的关注较少。比如，可以在大多数所学习的参数固定时，研究各种池化策略和残差连接的作用。